% !TEX root = chapter-standalone.tex

% \section{Computability of Design Problems}
\label{sec:computation}

Design problems are relations between different \SY{posets}, and they answer the question, ``can I provide a given functionality $f$ with resources $r$?''.

In engineering design, we are interested not only in whether a certain functionality can be provided with a given resource, but also in the \emph{minimal} amount of resources with which we can provide that functionality.
That is, given a required functionality, what are the \emph{minimal} sets of resources which provide it?

Let us define this query more precisely.
For any \SY{design problem} $\adp$, we can define a \SY{monotone function} $\ftor_\adp$ that sends a functionality $f \setin F$ to the \emph{minimum} \SY{antichain} of resources which provide~$f$:
\begin{equation}
    \begin{aligned}
        \ftor_\adp \colon F & \to \antichains R, \\
        f                   & \mapsto \Min \makeset{ r \setin R \colon \adp(f^*, r) = \true }.
    \end{aligned}
\end{equation}

\begin{definition}\label{def:dp-computable}
    A \SY{design problem} is \emph{computable} if an exact solution to $\ftor_\adp$ can be computed in time polynomial in the computation time of its component \SY{design problems}.
    If a \SY{design problem} has no component \SY{design problems}, then we say that it is \emph{primitive}.
\end{definition}

In this section, we assume that all primitive \SY{design problems} are computable, that is, $\ftor_\adp$ terminates in finite time.

% \AC{polynomial in what?} \JT{I wasn't sure, to be honest. I was hoping you could answer this. It seems like the most reasonable thing is to define it recursively it as ``time polynomial in the computation time of primitive \SY{design problems} '', and ``A primitive design problem $\adp$ is computable if $h_\adp$ is a computable function.'' which roughly follows the logic in your original paper. [Is this a problem? Kleene converges in enormous time! You can't put any bound on it. It doesn't necessarily converges in polynomial time??? Need to think of this more.]}

Is this map computable?
In this section we will show that the answer is ``no'' in general, and we will find sufficient conditions for the answer to be ``yes''.
The fact that \DP is compact closed, which we proved in~\XXX, will help us describe and solve these optimization problems.
\todotext{that is now moved in playground}

\subsection{Preliminaries on antichains}

We now use the \SY{antichains} machinery developed in~\cref{sec:chains-antichains}.
For \posA a poset, $\uppersets \posA$ denotes the set of \SY{upper sets}, and $\antichains \posA$ the set of \SY{antichains}.

% We denote the smallest upper set in $P$ containing $p$ by $\upit(p) = \makeset{ x \setin P : p \leq x } \setin \uppersets P$.

We will see $\antichains \posA$ itself as a poset, with the order given by
\begin{equation}
    S \posleqof{\antichains \posA} T \quad \equiv \quad\upit S\, \supset\,\upit T,
\end{equation}
where $\upit$ is the \SY{upper closure} of the set (\cref{def:upperset}).

The \SY{top element} of the \SY{poset} $\antichains\posA$ is the empty set $\Emptyset$.

When we talk about computing minimal sets of resources, we always mean computing an appropriate \SY{antichain}, \ie, an element in $\antichains \posA$.

\subsubsection{Sufficient conditions for bijection between DPs and antichains }

We \emph{almost} have a \SY{bijection} between \SY{design problems} and \SY{antichains}.

The natural \SY{bijection} would be
\begin{equation}
    \begin{aligned}
        \Hom_\DP(F,R) & \overset{?
        }{\leftrightarrow} \antichains (F\op \times R), \\
        \adp          & \mapsto \Min K_\adp,
    \end{aligned}
\end{equation}
where $K_d$ is the feasible set of $\adp$,
and the map $\Min$ returns the minimal elements of the subset~(\cref{def:Min}).

The problem is that not all \SY{upper sets} of \posA are representable by their minimal elements.
For example, take the two distinct \SY{upper sets} $S_1 = \Emptyset$ and $S_2 = \makeset{x \setin \reals : x > 0}$.
We have $\Min S_1 = \Min S_2 = \Emptyset$.

Suppose we restrict to only subsets which are \emph{downward-closed}, \ie, $\Min S = \Emptyset \Rightarrow S = \Emptyset$.
% \devel{
%     \AC{
%         I am not sure the above is what we want.
%         This is more explicit:

%         Suppose that we restrict to the upper sets that are downward-closed, in the sense
%         that
%         \begin{equation}
%             S = \ \upit \Min S.
%         \end{equation}

%         Are these equivalent?
%         In any case I put that definition now as \cref{def:downward-closed-upperset}.

%     } \JT{They're not equivalent but on upper sets they're very close.
%         Assume my definition, and let $\Min S = \Emptyset$, then $S = \Emptyset$, which does \emph{not} satisfy $S = \upit \Min S$.
%         On the other hand, suppose $\Min S = K \neq \Emptyset$ for some set $K$.
%         Then $S \neq \Emptyset$ trivially, and it remains to determine $\upit K$.
%         But since $S$ is an upper set, every $s \setin S$ satisfies $s \geq k$ for some $k \setin K$, so $s \setin \upit K$; on the other hand this is the same definition of a typical element $k \setin \upit K$.
%     }

%     So basically, with respect to all the proofs below (which involve only upper sets and exclude the case $\Emptyset$) both definitions work fine.
%     Mine feels a bit weaker (though neither implies the other due to how they behave on $\Emptyset$) but yours is maybe more clear.
%     So I'm happy to use yours.
% }

We say that a \SY{design problem} $\adp : F \profto R$ is \emph{downward-closed} if $K_d \subset F\op \times R$ is downward-closed.

% In effect, this means that the set of feasible functionalities (a lower set in $F$) is closed under taking maximal elements and that the set of feasible resources (an upper set in $R$) is closed under taking minimal elements.

\begin{lemma}
    There exists a \SY{bijection} between the set $\HomSet{\underline{\DP}}{F}{R}$ of downward-closed \SY{design problems} $\adp : F \profto R$ and the set $\antichains (F\op \times R)$ of \SY{antichains} of $F\op \times R$, %\AC{For the sets for which $\Min$, $\uppersets $ is a bijection, it is also an order isomorphism. I suspect this will be needed later.}
    \begin{equation}
        \begin{aligned}
            \Hom_{\underline{\DP}}(F,R) & \leftrightarrow \antichains (F\op \times R) \\
            \adp                        & \mapsto \Min K_\adp.
        \end{aligned}
    \end{equation}
\end{lemma}
\begin{proof}
    Every $\adp$ can be represented by its feasible set $K_\adp$, so $\Hom_\DP(F,R) \simeq \uppersets (F\op \times R)$.
    In effect, the proof comes down to the fact that \SY{antichains} are compact representations of \SY{upper sets} which are downward-closed.
    We will show that, for any \SY{poset} $P$, there is a \SY{bijection} between the \SY{antichains} of $P$ and the set of \SY{upper sets} of $P$ which are downward-closed.

    Pick $A \setin \antichains\posA$.
    Then $A$ generates an \SY{upper set} $\upit A = \makeset{ x \setin P : a \leq x \text{ for some $a \setin A$} }$, and $\Min \upit A = A$.
    From the other direction, consider $\dcuppersets \posA$, the set of \SY{upper sets} of \posA which are downward-closed.
    Pick a non-empty $U \setin \dcuppersets \posA$.
    Since $U$ is downward-closed, $\Min U$ is a non-empty \SY{antichain} given by $\makeset{ a \setin \underline{U} : a \leq x \text{ for all $x \setin \underline{U}$} }$, and $\upit \Min \underline{U} = \underline{U}$.
\end{proof}

\begin{remark}
    Another way of looking at the \SY{bijection}: a \SY{poset} $P$ considered with respect to only those \SY{upper sets} which are downward-closed is an approximation of a well-founded poset, a.k.a.
    a \SY{poset} which satisfies the descending \SY{chain} condition.
    In such \SY{posets}, there is a well-known \SY{bijection} between \SY{upper sets} and \SY{antichains} (assuming the axiom of dependent choice).
\end{remark}
\devel{
    \AC{I don't understand what "approximation" means here.} \JT{I wrote approximation because restricting attention to just those downward-closed \SY{upper sets} was less strong than just assuming DCC on the whole \SY{poset} (I think of it as focusing attention on parts of the \SY{poset} while leaving the funky other parts blurred out), but allowed you to prove a similar \SY{bijection}.
        But I'm not familiar with the arguments in terms of trees that Hamkin brought up, I'll think about this later.
    }

    \AC{
        By the way, when I asked, I was told that you need Choice
        to make the \SY{bijection} work.
        See \href{https://mathoverflow.net/questions/219425/acc-dcc-implies-upper-lower-sets-are-upper-lower-closure-of-antichains}{this discussion}.
    } \JT{I added the mention of choice in the remark.
        For the main proof see above comment.
    }
}
Why does this \SY{bijection} matter?

Looking at the definition of $\ftor_\adp$:
\begin{equation}
    \begin{aligned}
        \ftor_\adp \colon F & \to \antichains R \\
        f                   & \mapsto \Min \makeset{ r \setin R \colon \adp(f^*, r) = \true }
    \end{aligned}
\end{equation}
%
Assuming $K_d \neq \Emptyset$, one immediately observes that an exact solution exists if and only if $\adp$ is downward-closed.
So the \SY{bijection} tells us how to restrict to \SY{design problems} which have exact solutions.
% Unfortunately, this bijection still has problems. Imagine if we take a subset with two minimal elements and then remove one of them. There may still be a set of feasible solutions in a neighborhood of the removed element that are not dominated by the leftover minimal solution, but $\Min$ would miss these solutions. It feels like we should also discount these kinds of situations?

\subsubsection{Computability of  design problems}

A \emph{primitive} \SY{design problem} is any \SY{design problem} which is not constructed using any of the operators we have already defined.
A primitive \SY{design problem} $\adp$ is computable when $\ftor_\adp$ is computable in polynomial time.
% Almost any primitive design problem $\adp$ is computable in polynomial time, since computing $h_\adp$ amounts to querying a lookup table.
\devel{
    \AC{for the above: what is a primitive design problem?} \JT{See above.}

    \AC{And what does it mean almost?} \JT{See above.}

    \AC{"querying" a lookup table might take infinitely long if such lookup table is infinite...} \JT{Fair enough.
        I've changed the characterization.
    }
}
Every combination of computable \SY{design problems} we have so far looked at---excepting loop---can be easily seen to be computable:

\begin{proposition}
    The composition, monoidal product, and coproduct of computable \SY{design problems} are also computable.
\end{proposition}

\begin{proof}
    These are direct properties of \DP.

    \todotext{ \AC{This is not really a proof, it is just a statement of facts without proofs.} \JT{Hmm I thought I was copying a proposition in your original paper but now that I go back I can't find it.
            In that case I will need to think about this a bit more.
            The issues are the $\Min$ in the composition and the coproduct in the cases with an infinite \SY{antichain}.
        }}

    \textbf{Composition}.
    Suppose $\adp_1 : A \profto B$ and $\adp_2 \colon B \profto C$.
    Then $h_{\adp_1\dpthen \adp_2} $ is defined by
    \begin{equation}
        \begin{aligned}
            h_{\adp_1 \dpthen \adp_2} \colon A & \to \antichains C \\
            a                                  & \mapsto \Min \bigsetunion_{b \setin h_{\adp_1}(a)} h_{\adp_2}(b).
        \end{aligned}
    \end{equation}
    Clearly this is computable if $h_{\adp_1}, h_{\adp_2}$ are computable.

    \todotext{\AC{Is it obvious that $\Min$ itself is computable?} \JT{See above.}}

    \textbf{Monoidal product}.
    Similarly with the monoidal product: if $\adp_1 \colon A \profto C$ and $\adp_2 \colon B \profto D$, then $h_{\adp_1 \otimes \adp_2}$ is defined by
    \begin{equation}
        \begin{aligned}
            h_{\adp_1 \otimes \adp_2} \colon (A \times B) & \to \antichains (C \times D) \\
            \langle a, b \rangle                          & \mapsto h_{\adp_1}(a) \times h_{\adp_2}(b).
        \end{aligned}
    \end{equation}

    \textbf{Coproduct}.
    Similarly with the coproduct, for $\adp_1, \adp_2 : A \profto B$ below:
    \begin{equation}
        \begin{aligned}
            h_{\adp_1 + \adp_2} \colon A & \to \antichains B \\
            \tup{a,b}                    & \mapsto \Min (h_{\adp_1}(a) \setunion h_{\adp_2}(b)).
        \end{aligned}
    \end{equation}

    \todotext{\AC{This last one should be $\adp_1 + \adp_2$, not $\adp_1 \sqcup \adp_2$, right? } \JT{I think so; I'm not sure what the most recent notation is.}}
\end{proof}

Loops are significantly harder.
To see that we can compute them in finite time, we will need the following fact:
\begin{lemma}
    If a \SY{design problem} $\adp : F \profto R$ is downward-closed, then $\ftor_\adp$ is \SY{Scott continuous}.
    % Weaker version: If a design problem $\adp : F \tickar R$ is downward-closed, and $(f^*,r)$ is a minimal element of $K_\adp$, then $h_\adp$ is Scott-continuous at $f$, \ie  if $f = D^\upit$ is the supremum of some directed subset $D \subset F$, then $h_\adp(f) = h_\adp(D)^\upit$.
\end{lemma}

\begin{proof}
    Recall that a \SY{monotone function} is \SY{Scott continuous} if it preserves all directed suprema, \ie, if $D$ is a \SY{directed subset} and $D^{\upit}$ is its \SY{supremum}, then $f(D^{\upit}) = f(D)^{\upit}$.
    $\adp$ \SY{Scott continuous} just means that the \SY{supremum} of any feasible, \SY{directed subset} must also be feasible.
    If $\adp$ is downward-closed, then $\adp$ is \SY{Scott continuous}.

    \todotext{\AC{I am confused.
            In "$\adp$ \SY{Scott continuous} just means.
            ..", what does it
            mean for a DP to be Scott continuous?
            \SY{Scott continuous} is a property of a map.
        } \JT{Recall that $\adp$ is defined to be a \SY{monotone map} from a given \SY{poset} $F\op \times R$ to \Bool.
            The sentence above is a direct translation of what it means for $\adp(D^{\upit}) = \adp(D)^{\upit}$, where $D \subset F\op \times R$.
        }}

    If $\adp$ is \SY{Scott continuous}, then $\ftor_\adp$ is \SY{Scott continuous}, since $\ftor_\adp$ only observes the lower boundary of the feasible set, which is controlled by the downward-closed condition.
\end{proof}

For a \SY{design problem} of the form $\adp : (B \times A) \profto (B \times C)$, $\ftor_\adp : B \times A \to \antichains (B \times C)$ is defined as normal.
For any input $a$ we define a recursion operator $\psi_{a,\adp}$ for $\ftor_\adp$ by
\begin{equation}
    \begin{aligned}
        \psi_{a,\adp} \colon \antichains (B \times C) & \to \antichains (B \times C) \\
        S                                             & \mapsto \Min \bigsetunion_{\tup{b,c} \setin S} \makeset{ \tup{b',c'} \setin \ftor_\adp(b,a) \colon b' \leq b }.
    \end{aligned}
\end{equation}
%\begin{align*}
%\psi_{a,\adp} : \antichains (B) &\to \antichains (B) \\
%S &\mapsto \Min \bigsetunion_{b \setin S} \makeset{ b' \setin B : \langle b',c \rangle \setin h_\adp(b,a), b' \leq b }.
%\end{align*}

In that case, $h_{\Tr \adp}$ is defined by
\begin{equation}
    \begin{aligned}
        h_{\Tr \adp} \colon A & \to \antichains C \\
        a                     & \mapsto \Min \pi_C(\text{lfp}(\psi_{a,\adp})).
        \label{eq:magic}
    \end{aligned}
\end{equation}
where $\pi_C \colon B \times C \to C$ is the projection on $C$.
\devel{
    \AC{TODO: define  $\pi_C$ (projection on $C$)} \JT{Done.}

    \AC{Where does \text{lfp} come from?
        I think there are many passages omitted here.
    } \JT{I assume they know what a lfp is, or can look it up.}

    \AC{Also, this formula is not equivalent to what I had found; see below for more comments.} \JT{The main thing that is different is the fact that we're carrying around an extra variable's worth of data, namely $C$.}}
%\begin{align*}
%h_{\Tr \adp} : A &\to \antichains C \\
%a &\mapsto \Min \pi_C \left ( \Min \bigsetunion_{b \setin \text{lfp}(\psi_{a,\adp})} h_\adp(b,a) \right ).
%\end{align*}

To say that $\Tr \adp$ is computable is to say that the \SY{least fixed point} $\text{lfp}(\psi_{a,\adp})$ exists.

What's going on here?

\begin{enumerate}
    \item First, calculate $\ftor_\adp$ for some fixed, initial $\langle b_{\text{in}},a \rangle$.
          The goal is to compute the \SY{antichain} of minimal elements $c \setin C$ which provide $a$, across \emph{all} possible choices of $b_{\text{in}}$.
    \item Throw away all those minimal elements in $\ftor_\adp(b_{\text{in}},a)$ that do not satisfy the trace constraint, $b_{\text{in}} \leq b_{\text{out}}$,\footnote{If the loop is composed with an addition behind the input, which is often the case, then this condition would change to reflect the addition.} where $b_{\text{out}} \setin \pi_B(\ftor_\adp(b_{\text{in}},a))$.
    \item At this point, we are left with an \SY{antichain} $S \subset \antichains (B \times C)$ of minimal elements $\langle b_{\text{out}},c \rangle$ that satisfy the trace constraint, but the resources $c \setin S|_C$ (which do not necessarily form an \SY{antichain}) are not necessarily minimal with respect to the functionality $a$, since we might have chosen some other initial $b_{\text{in}}$ and gotten better results.
          Alternately, if we had started with the wrong $b_{\text{in}}$, the entire computation might have terminated with $\Emptyset$.
          % [Andrea's example in the original paper seems to imply that the antichains are not necessarily feasible, rather than minimal.]
    \item Take the list of $b_{\text{out}}$ from Step 2 as the new $b_{\text{in}}$, and run through steps 1--2 in parallel for each new $b_{\text{in}}$.
          Take the union of the results, and take the $\Min$ of the union to obtain a new list of $b_{\text{out}}$.
          Each result will be a smaller \SY{antichain} in $C$, \ie, a ``better'' result.
          % Note that this requires that the minimal elements exist for arbitrary subsets, not just $K_\adp$. Motivates dcpo.
    \item Keep iterating Step 4 until the result converges to a \SY{least fixed point}: the \SY{antichain} of minimal elements $\langle b, c \rangle \setin B \cartprod C$ which provide $\langle b, a \rangle$, across \emph{all} possible choices of $b_{\text{in}}$.
    \item Lastly, given an \SY{antichain} in $\antichains (B\times C)$, we need to convert it into an \SY{antichain} $\antichains C$.
          The obvious way to do this is to project the \SY{antichain} to $C$ and take the minimal elements of the projection.

          % Typically, these results will look something like, assuming the biggest allowable b, find the smallest resources c.
\end{enumerate}
\devel{
    \JT{Something about this last step feels wrong.
        I feel that $\antichains (B \times C)$ is more important than $\antichains C$.
        Wouldn't you prefer to retain the information there, rather than throw it all away?
        Projecting to the minimal elements of the projection would also distinguish a subset of the ``stable'' $b$, namely those in the pre-image under the projection of those minimal elements (pullback?
        ).
    }

    \AC{
        In fact, I don't think that \cref{eq:magic} is correct.
        There are some gaps
        in the derivation.
        Try to fill those gaps and see if the result changes?
    } \JT{I'll look at it again.
        If you think of something specific, could you help me by pointing it out?
        In reference to ``magic equation'', see comment above.
    }
}
%\JT{Never mind, this is definitely not true. Imagine if $B = \makeset{*}$.}
%We can simplify this problem by first decomposing the traced design problem $\Tr \adp$ into a composition $\Tr \adp_1 ; \adp_2$, for some \SY{design problems} $\adp_1$ and $\adp_2$:
%\[
%\begin{tikzpicture}[oriented WD, bb min width =.7cm, bby=2ex, bbx=.7cm,bb port length=3pt]
%    \node[bb port sep=.8, bb={2}{2}, bb name={$\adp$}] (Loop) {};
%    \draw[ar] let \p1=(Loop.north east), \p2=(Loop.north west), \n1={\y1+\bby}, \n2=\bbportlen in (Loop_out1) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (Loop_in1);
%        \node[label, left=0.2 of Loop_in1] {$B$};
%        \node[label, left=0.2 of Loop_in2] {$A$};
%        \node[label, right=0.2 of Loop_out1] {$B$};
%        \node[label, right=0.2 of Loop_out2] {$C$};
%\end{tikzpicture}
%\quad
%=
%\quad
%\begin{tikzpicture}[oriented WD, bb min width =.7cm, bby=2ex, bbx=.7cm,bb port length=3pt]
%   \node[bb port sep=.8, bb={2}{1}, bb name={$\adp_1$}] (Loop) {};
%    \draw[ar] let \p1=(Loop.north east), \p2=(Loop.north west), \n1={\y1+\bby}, \n2=\bbportlen in (Loop_out1) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (Loop_in1);
%        \node[label, left=0.2 of Loop_in1] {$B$};
%        \node[label, left=0.2 of Loop_in2] {$A$};
%        \node[label, below right=0.4 and 0.3 of Loop_out1] {$B$};
%    \node[bb port sep=.8, bb={1}{1}, right=1.5 of Loop, bb name={$\adp_2$}] (C) {};
%    \draw[ar] (Loop_out1) to (C_in1);
%        \node[label, right=0.2 of C_out1] {$C$};
%\end{tikzpicture}
%\]

% Here, the fact that $h_\adp$ is monotone is essential; it means that we can take the output $b_{\text{out}}$'s, put them as the new inputs, and then iterate the process above with each of the new $b_{\text{out}}$'s. We are guaranteed an antichain in $C$ which is greater than or equal.

% To define $h_{\Tr \adp}$ over the trace $\Tr \adp_1 : A \tickar B$, we need to minimize over different versions of $h_\adp$, where each version is parameterized over a fixed element of $B$. This parameterized space of $h_\adp$'s is equivalent to $\antichains (B)$, \ie  each antichain of elements in $B$ is also an equivalence class of elements that are the \emph{minimal} elements which provide some functionality $a$. \JT{If we're going to use \DP compact closed, we probaly ought to use it here. What's the most reasonable way?}

% Note that $\psi_{a,\adp}$ is a \SY{monotone function} since $h_\adp$ is. Given the value of the input functionality $a$, $\psi_{a,\adp}$ first computes $h_\adp(b, a)$, then checks the trace constraint $b^* \leq b$ by intersecting the set $h_\adp(b,a)$ with $\upit(b)$. This done for all $b \setin S$, and the output is unioned: this constructs the total feasible set $K_S$ for the antichain $S$, modulo the trace constraint. We then compute the minimal elements of this feasible set.

% $\psi_{a,\adp}$ minimizes $h_\adp(b, a)$ along the looped variable $B$. This returns an antichain in $B$ satisfying the trace constraint, but this antichain is not necessarily the minimum one in $B$ with respect to the set of all feasible antichains. If not, we iterate $\psi_{a,\adp}$ until we reach a least fixed point.

To ensure that the \SY{least fixed point} exists, we require that the \SY{posets} $B, C$ (thus $\antichains (B \times C)$) be a \emph{directed complete partial order} or \emph{dcpo}, \ie, that there exists a \SY{supremum} for any \SY{directed subset} $S \subset \antichains (B \times C)$.
To ensure that we do not miss any solutions by starting with the wrong $b_{\text{in}}$, we also require that $B$ be \emph{pointed}, \ie, that it possess a unique \SY{bottom element}.

% We also require that $h_\adp$ be Scott-continuous, \ie  that it preserve these suprema.

\begin{lemma}
    If $\ftor_\adp$ is \SY{Scott continuous}, then so is $\psi_{a,\adp}$.
\end{lemma}

\begin{proof}
    The statement is Exercise 8.26 in Davey and Priestly \citeXXX.
\end{proof}

% Since \DP is compact closed, and in fact, by the lemma above, $\Hom_\underline{\DP}(A,B) \simeq \antichains (A\op \times B)$, we know that this is possible if we restrict to $\underline{\DP}$.

%\begin{figure}[h!]
%\centering
%\includegraphics[width=\textwidth]{pics/scottcontinuous}
%\end{figure}

% $\antichains B$ with a bottom element means that $B$ has to have all suprema, \ie  be upward-closed.

\begin{proposition}
    If $\adp$ is downward-closed and $\antichains (B \times C)$ is a pointed dcpo, then $\Tr \adp$ is computable.
\end{proposition}

\begin{proof}
    The proposition is a direct application of Ad\'amek's theorem for algebras over an \SY{endofunctor}.

    Recall the statement of Ad\'amek's theorem for algebras over an \SY{endofunctor}: let \CatC be a category with an \SY{initial object} $0$ and transfinite composition of length $\omega$, hence colimits of sequences $\omega \to \CatC$ (where $\omega$ is the first infinite ordinal), and suppose $F: \CatC \to \CatC$ preserves colimits of $\omega$-chains.
    Then the colimit $\gamma$ of the \SY{chain} \begin{equation}
        0 \overset{i}{\to} F(0) \overset{F(i)}{\to} \ldots \to F^{(n)}(0) \overset{F^{(n)}(i)}{\to} F^{(n+1)}(0) \to \ldots
    \end{equation}
    carries the structure of an initial $F$-algebra.

    The category \CatC is the \SY{poset} $\antichains (B \times C)$, whose \SY{initial object} is just the \SY{bottom element} $\bot$.
    The colimit of a (possibly infinite) sequence or \SY{chain} of elements in $\antichains (B\times C)$ is simply the \SY{supremum} of that \SY{chain}.
    The \SY{endofunctor} $F$ is given by the recursion operator $\psi_{a,\adp} : \antichains (B\times C) \to \antichains (B\times C)$, and by the previous lemmas, $\psi_{a,\adp}$ is \SY{Scott continuous}, thus preserves suprema.
    Finally, under repeated applications of $\psi_{a,\adp}$, the colimit $\gamma$ (now defined in \Pos as opposed to $\antichains B$) of that sequence of \SY{monotone maps} is the fixed point of $\psi_{a,\adp}$; the fact that it is an initial $F$-algebra---\ie, an element $S$ in the \SY{poset} $\antichains (B\times C)$ that is ``initial'', \ie, $S \leq T$ for all $T \setin \antichains B$---means that $\gamma$ is the \SY{least fixed point}.
\end{proof}
\devel{
    \AC{I'm trusting you on the above.}
}
Directed complete partial orders and \SY{Scott continuous} functions define their own category of \SY{design problems}, $\DP_S$.
We can think of $\DP_S$ as the \SY{subcategory} of \DP whose morphisms preserve all sequential colimits.

%\JT{Downward-closed $\adp$ means that $h_\adp$ has an exact solution. Can it say anything more? I would like to prove $\adp$ downward-closed implies $h_\adp$ Scott-continuous, but I'm not sure that result is true. Downward-closed is a local property about a particular subset of $F\op \times R$, namely $K_\adp$. $h_\adp$ being Scott-continuous is a global property about $F \times \antichains R$ that says something about \emph{all} directed subsets of $F \times \antichains R$ (so really about $\mathcal{P}(F) \times \mathcal{P}(\antichains R)$).
%
%Is there a local-global characterization of Scott-continuous? Maybe I could do something if I proved something about the class / category of all downward-closed \SY{design problems}. This is the same category as \DP minus certain morphisms. Or maybe Markowsky's theorem could be useful---it states that every well-founded chain-complete \SY{poset} is a dcpo.%see https://projects.lsv.ens-cachan.fr/topology/?page_id=563
%
%The point is, we need $\psi_{a,\adp}$ to be Scott-continuous. Well, we already assume that our \SY{design problems} are downward-closed. So how close does this assumption get us to the thing we need, \ie  $\psi_{a,\adp}$ to be Scott-continuous? Does this imply that all \SY{posets} are dcpos and all $\adp$ are Scott-continuous?}

%Let the projections
%\[F_\adp\op = \pi_{F\op}(K_\adp), \quad R_\adp = \pi_R(K_\adp)\]
%be the set of feasible functionalities and the set of feasible resources, respectively.

% Review of Kleene ascent: high-level: start from bottom, iterate until you get to the least fixed point. [Kleene ascent is basically just gradient descent.]

% It is also common to expect to find several solutions that do not nominate each other. Therefore, rather than looking for the ``best'' solution, what one can expect to find is a trade-off curve, or ``Pareto frontier'' of solutions.

\subsubsection{Duality}

We can also define the dual problem; suppose we fix a given resource $r \setin R$ and ask for the \emph{maximal \SY{antichain} } of functionalities that can be provided with $r$.
Then we can define a function
\begin{equation}
    \begin{aligned}
        g_\adp : R & \to \antichains F\op \\
        r          & \mapsto \Min \; \makeset{ f^* \setin F\op : \adp(f^*, r) = \true }
    \end{aligned}
\end{equation}
with analogous properties to $\ftor_\adp$.
\devel{
    \AC{Do we know if $\ftor_\adp$ computable implies $g_\adp$ computable?} \JT{I have not checked and am fine with removing this comment in the interests of time.}
}

\subsubsection{To put back somewhere}
Note that $\ftor_\adp$ is a \SY{monotone function} but is \emph{not} a design problem.
Given that it is a \SY{monotone function}, what happens if we try to turn it into a design problem?
The \SY{conjoint} of $\ftor_\adp$ is given by
\begin{equation}
    \begin{aligned}
        \hat \ftor_\adp : F    & \profto \antichains R \simeq \Hom_\DP(\ast, R) \\
        \langle f^*, S \rangle & \mapsto \antichains R(\ftor_\adp(f), S) \leftrightarrow \bigwedge_{s\setin S} \adp(f^*, s).
    \end{aligned}
\end{equation}

In other words, the \SY{conjoint} of $\ftor_\adp$ is an indicator function for subsets of feasible resources.
